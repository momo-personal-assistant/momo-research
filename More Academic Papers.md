# More Academic-Leaning Papers
These papers might be less relevant for an engineered solution to memory but are still interesting and highly significant in the field!

## On the Theoretical Limitations of Embedding-Based Retrieval
[Paper](https://www.alphaxiv.org/abs/2508.21038)

This research formally proves that single-vector embedding models possess fundamental theoretical and practical limitations in representing complex, combinatorial relevance definitions, irrespective of model size or training data. It demonstrates that these constraints manifest in realistic scenarios, causing state-of-the-art models to struggle on specially designed stress-test datasets. Alternative architectures like cross-encoders, sparse models, and multi-vector models can overcome these limitations, suggesting that architectural innovation beyond the single-vector paradigm is crucial for advanced retrieval tasks.

## Titans: Learning to Memorize at Test Time
[Paper](https://www.alphaxiv.org/abs/2501.00663)

Titans introduces a new neural memory module that learns to memorize information at test time and effectively combines with attention mechanisms for better long-term sequence modeling.

## Nested Learning: The Illusion of Deep Learning Architecture
[Paper](https://abehrouz.github.io/files/NL.pdf)

This paper introduces a new learning paradigm called Nested Learning (NL) that represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems. NL provides a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. The authors present three core contributions: Expressive Optimizers, Self-Modifying Learning Module, and Continuum Memory System, and demonstrate promising results in language modeling, knowledge incorporation, and few-shot generalization tasks.

## Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences
[Paper](https://www.alphaxiv.org/overview/2509.16189)

Research from Google DeepMind and Stanford University demonstrates that current parametric AI systems lack the ability for latent learning, struggling to flexibly reuse implicitly acquired information for new, uncued tasks. Implementing an episodic memory-like retrieval mechanism consistently and significantly improves generalization across various benchmarks, suggesting it complements parametric learning by enabling on-demand, in-context reuse of past experiences.